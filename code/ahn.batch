#!bash

#Load needed R
module load R/3.3.3
tempMothur=/nfs/turbo/pschloss/msze/axiom_home_stuff/bin/mothurV1.39.3/mothur/mothur
DOWNDIR=data/raw/ahn
WORKDIR=data/process/ahn
REF=data/references

# Need to add in a bunch of stuff on the study and what they were looking for.
# Might adjust parameters to those set by authors of original manuscript
# (Geng, J et al 2013).  mothur default for 454 allowed only an average of
# 855 sequence per sample. Author method leave ~1,190 sequences per sample. 
# The SRA project ID is SRP016877.

# Download the data set
	# Need to obtain permission from dbgap and then download the data from there
	# SRP057278 
	# Metadata is public and is provided.

# Unzip the data
#tar -C $DOWNDIR -xvf $DOWNDIR/dbgap_seqs.tar
#tar -C $DOWNDIR -xvf $DOWNDIR/metadata.tar
#gzip -d $DOWNDIR/*.gz
#mv $DOWNDIR/phs000884.v1.pht004601.v1.p1.c1.Gut_Microbiome_Controls_Subject_Phenotypes.HMB-MDS.txt $WORKDIR

# Convert to fasta files that will be used
#for sample in $DOWNDIR/*.fastq
#do
#	$tempMothur "#fastq.info(fastq=$sample);
#		trim.seqs(fasta=current, qfile=current, maxambig=0, maxhomop=8, qwindowaverage=25, qwindowsize=50, processors=8)"

#	rm *logfile
#done

# Combined the seperate fasta files  to one file
#cat $DOWNDIR/*trim.fasta > $WORKDIR/combined.fasta

# Create a group file
#grep '^>' $WORKDIR/combined.fasta | cut -c 2- > $WORKDIR/header.txt
#sed 's/\..*//g' $WORKDIR/header.txt > $WORKDIR/group.txt
#paste --delimiters='\t' $WORKDIR/header.txt $WORKDIR/group.txt > $WORKDIR/combined.groups

# Remove unessary files
#rm -f $DOWNDIR/*.pdf $DOWNDIR/*.xml $DOWNDIR/*.txt
#rm -f $DOWNDIR/*.qual $DOWNDIR/*.fasta $DOWNDIR/*.fastq $DOWNDIR/*.fastq.gz
#rm -f $WORKDIR/header.txt $WORKDIR/group.txt

# Run mothur for sequencing processing on combined file
#$tempMothur "#unique.seqs(fasta=$WORKDIR/combined.fasta);
#	align.seqs(fasta=current, reference=$REF/silva.seed.align, flip=T, processors=8);
#	count.seqs(name=current, group=$WORKDIR/combined.groups);
#	summary.seqs(fasta=current, count=current);
#	screen.seqs(fasta=$WORKDIR/combined.unique.align, count=$WORKDIR/combined.count_table, start=6451, optimize=end, criteria=95, minlength=200, maxhomop=8, processors=8);
#	filter.seqs(fasta=current, vertical=T, trump=.);
#	unique.seqs(fasta=current, count=current);
#	summary.seqs(fasta=current, count=current);
#	pre.cluster(fasta=current, count=current, diffs=2);
#	chimera.uchime(fasta=current, count=current, dereplicate=t);
#	remove.seqs(fasta=current, accnos=current);
#	classify.seqs(fasta=current, count=current, reference=$REF/trainset14_032015.pds.fasta, taxonomy=$REF/trainset14_032015.pds.tax, cutoff=80);
#	remove.lineage(fasta=current, count=current, taxonomy=current, taxon=Chloroplast-Mitochondria-unknown-Archaea-Eukaryota);
#	summary.seqs(fasta=current, count=current);
#	cluster.split(fasta=current, count=current, taxonomy=current, method=opti, metric=mcc, taxlevel=5, cutoff=0.03);
#	make.shared(list=current, count=current, label=0.03);
#	classify.otu(list=current, count=current, taxonomy=current, label=0.03);
#	get.oturep(fasta=current, count=current, list=current, label=0.03, method=abundance)"

# Match metadata with the shared file
#R -e "source('code/ahn.R')"

#$tempMothur "#sub.sample(shared=$WORKDIR/ahn.shared, size=1000, label=0.03);
#	dist.shared(shared=$WORKDIR/ahn.shared, calc=braycurtis, label=0.03, subsample=1000, iters=100, processors=8);
#	summary.single(shared=$WORKDIR/ahn.shared, calc=nseqs-sobs-shannon-shannoneven, subsample=1000)"

mv $WORKDIR/*.cons.taxonomy $WORKDIR/ahn.taxonomy
mv $WORKDIR/*0.03.rep.fasta $WORKDIR/ahn.rep.seqs

rm $WORKDIR/combined.*
rm $WORKDIR/*rabund





